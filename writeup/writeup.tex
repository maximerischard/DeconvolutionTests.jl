\documentclass[12pt]{article}

\usepackage[unicode=true,bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=page,colorlinks=true]{hyperref}

% for references
\def\sectionautorefname{Section}
\def\Appendixautorefname{Appendix}
\def\thmautorefname{Theorem}
\def\corolcounterautorefname{Corollary}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\subsubsubsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\equationautorefname~#1\null{(#1)\null}

\usepackage{dsfont}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{bm}
\usepackage[toc,page]{appendix}


% pseudo code
\usepackage[ruled]{algorithm2e}

\def\algorithmautorefname{Algorithm}


\usepackage{natbib}

\usepackage{geometry}
\graphicspath{{../figures/}}
% \geometry{margin=1.25in}
\linespread{1.5}


\makeatletter
\newcommand*{\iid}{%
    \@ifnextchar{.}%
        {i.\,i.\,d}%
        {i.\,i.\,d.\@}%
}
\makeatother
\newcommand{\mathiid}{\text{\iid}}
\newcommand{\simiid}{\overset{\mathiid}{\sim}}

% General macros
\newcommand{\genericdel}[3]{%
  \left#1{#3}\right#2
}
\newcommand{\del}[1]{\genericdel(){#1}}
\newcommand{\sbr}[1]{\genericdel[]{#1}}
\newcommand{\cbr}[1]{\genericdel\{\}{#1}}
\newcommand{\abs}[1]{\genericdel||{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator{\Pr}{\mathbb{p}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Ind}{\mathbb{I}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\ones}{\mathbf{1}}
% \DeclareMathOperator*{\argmin}{arg\,min}
% \DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\normal}{\mathcal{N}}
\DeclareMathOperator{\unif}{Uniform}
\DeclareMathOperator{\poisson}{Poisson}
\DeclareMathOperator{\dif}{d}
\newcommand{\od}[2]{\frac{\dif{#1}}{\dif{#2}}}
\DeclareMathOperator{\Forall}{\forall}
\newcommand{\indep}{\perp}
\newcommand{\trans}{^{\intercal}}
\newcommand{\reals}{\mathbb{R}}


% Macros specific to this paper %
\newcommand{\power}{\beta}
\newcommand{\alldistribs}{\mathcal{F}}
\newcommand{\Xtilde}{\tilde{X}}
\newcommand{\Ytilde}{\tilde{Y}}
\newcommand{\decon}{\mathtt{decon}}
\newcommand{\Fhat}{\hat{F}}
\newcommand{\pdiff}{\delta}

% Add a color for comments
\newcommand{\lfc}[1]{\textcolor{Lblue}{{\bf[LFC]} #1}}
\definecolor{Lblue}{rgb}{0.13, 0.36, 0.51}

\newcommand{\mr}[1]{\textcolor{Mgreen}{{\bf[MR]} #1}}
\definecolor{Mgreen}{rgb}{0.24, 0.51, 0.25}


\title{Testing for Equality of Distributions Under Known Additional Measurement Error}

\author{Luis F Campos, Maxime Rischard}

\date{\today}
\begin{document}


\maketitle

\lfc{This is written extremely loosely for now, just want to get everything down}

\section{Introduction} % (fold)
\label{sec:introduction}


{\bf{Known measurement error in Astronomy and related fields}}:

In the field of Astronomy, Astrophysics, etc. many, if not most, of the measurements astronomers use to answer questions have known measurement error. This error is known (or approximately known) because extensive testing is done on the ground to understand the behavior of potential observations when the observatory goes live. Typically, the data is analyzed without regard to this error, or is accounted for in rudimentary ways, such as reweighing. 


{\bf{The specific problem we're thinking about}}:

A problem was brought to us that is commonly found in Astronomy, we have a set of measurements coming from two different sets of galaxies and we want to know if the distributions of these measurements are the same. What is typically done is the measurements are collected and the a simple Kolmogorov-Smirnov Test (\cite{smirnov1948}) is conducted for equality of the distributions. The measurement error remains unaccounted for and invalidates the K-S test when the error is heteroskedastic, or dependent. We seek to find ways to analyze this type of data accounting for known measurement error that is generalizable and theoretically-grounded. But we will confine ourselves to the context of Hypothesis testing for equality of distributions for now.


{\bf{Our Strategy}}:

Generally, we use deconvolution (cite) in conjunction with parametric bootstrap to account for known measurement error distributions, then we use simple distributional distance metrics to test the equality of distributions. This strategy is helpful in several ways. The user can select different deconvolution techniques depending on the assumptions about the underlying process they are willing to make. The performance of different deconvolution techniques is dictated by the data (sample size, true distribution) and the measurement error distributions (smooth, super-smooth). Using a parametric bootstrap allows the user to define the measurement error distributions for their specific instrument and measurement. In some situations, practitioners happily assume symmetric super-smooth measurement error distributions, like the Normal distribution. But there are situations, e.g. real positive outcomes, where the measurement errors are not symmetric where other models are better fits. This can be accounted for in the parametric bootstrap since we simulate the error process and can implement it with any known distribution. The strategy also allows us to generalize away from any one test, like the K-S test, by enabling the user to define a test statistic that measures differences in distribution. Giving the user the freedom to select the most meaningful measure of difference for their specific problem. 

% section introduction (end)

\section{Problem Set-Up} % (fold)
\label{sec:problem_set_up}

In this section we set notation and the problem of distributional testing with known measurement error. 
We assume the following data generating process for our data. 

Denote the noise-free outcomes $X_i$, $i = 1,\dotsc,n_x$ and $Y_j$, $j=1,\dotsc,n_Y$:
\begin{equation}
\begin{split}
    X_i &\simiid F_X,\quad F_X \in \alldistribs \,, \\
    Y_j &\simiid F_y,\quad F_Y \in \alldistribs \,,
\end{split}
\end{equation}
where $\alldistribs$ is the set of all continuous and univariate probability measures.

We do not directly observe the noise-free outcomes, instead we measure the noisy observations 
\begin{equation}
    \Xtilde_i = X_i + \epsilon_{X,i}
    \quad\text{and}\quad
    \Ytilde_j = Y_j + \epsilon_{Y,j}.
    \label{eq:xytilde}
\end{equation}
The distribution of the errors are known, for example they can be assumed to be normal
\begin{equation}
    \epsilon_{X,i} \simiid \normal\del{0, \sigma^2_{X,i}}
    \quad\text{and}\quad
    \epsilon_{Y,j} \simiid \normal\del{0, \sigma^2_{Y,j}}
\end{equation}
with known variances $\sigma^2_{X,i}$ and $\sigma^2_{Y,j}$. Let's collect the known variances into vectors as
$$\sigma_X = (\sigma_{X,1}, \sigma_{X,2}, ..., \sigma_{X,n_X})
    \quad\text{and}\quad
\sigma_Y = (\sigma_{Y,1}, \sigma_{Y,2}, ..., \sigma_{Y,n_Y}).$$

We want to test the equality of the distributions from which the noise-free observations are drawn $H_0: F_X = F_Y$. 
A more formal statement of the hypotheses is:
\begin{equation}
\begin{split}
    & (F_X, F_Y) \in \alldistribs \times \alldistribs \equiv \Omega \\
    H_0:& (F_X, F_Y) \in \Omega_0,\quad \Omega_0 = \cbr{(F_A, F_B) \in \alldistribs \times \alldistribs,\ s.t.\ F_A =    F_B } \\
    H_1:& (F_X, F_Y) \in \Omega_1,\quad \Omega_1 = \cbr{(F_A, F_B) \in \alldistribs \times \alldistribs,\ s.t.\ F_A \neq F_B } \\
\end{split}
\end{equation}
which makes it clearer that the hypotheses partition the joint space $\alldistribs \times \alldistribs$
with $\Omega_0 \cap \Omega_1 = \emptyset$ and $\Omega_0 \cup \Omega_1 = \alldistribs \times \alldistribs$.

Our goal is to devise a hypothesis test which can distinguish $H_0$ and $H_1$.
We encode the hypothesis test with a test function $\varphi: \reals^{n_X} \times \reals^{n_Y} \to \{0, 1\}$
which takes in the noisy observations and returns either 0 (the null hypothesis is chosen) or 1 (the alternative is chosen).
The test function can also make use of the known noise variances $\sigma_X^2$ and $\sigma_Y^2$, but we suppress this in the notation for concision.
We further define the power of the test:
\begin{equation}
    \power(F_X, F_Y) = \E_{F_X,F_Y}\sbr{\varphi(\Xtilde, \Ytilde)}
\end{equation}
where $\E_{F_X,F_Y}$ denotes expectations with respect to $X$, $Y$, $\epsilon_X$ and $\epsilon_Y$.
We also define the significance
\begin{equation}
    \alpha = \sup_{(F_X,F_Y) \in \Omega_0}\del{ \power(F_X, F_Y) } = \sup_{F \in \alldistribs}\del{ \power(F, F) }\,,
    \label{eq:significance}
\end{equation}
the supremum of the power under the null hypothesis.
Under the classical hypothesis testing framework, we choose a nominal significance $\alpha^*$,
and then design a test function $\varphi$ which maximizes the power while satisfying $\alpha < \alpha^*$.

Under homoskedastic noise, i.e. $\sigma^2_{X,i} = \sigma^2_{Y,i} = \sigma^2$, the distributions of the noisy observations are the same under the null distribution and hence the K-S statistic is valid, though it will lose power as the noise increases. However, under heteroskedastic noise this will not be the case. We use alternative methods for testing this hypothesis. 

\section{Methods} % (fold)
\label{sec:methods}

\subsection{Deconvolution} % (fold)
\label{sub:deconvolution}
In this section we describe the deconvolution problem, some methods for performing deconvolution, and concerns in using this for Hypothesis testing. 
% subsection deconvolution (end)

\subsection{Two-sample tests for equality of distributions} % (fold)
\label{sub:two_sample_tests_for_equality_of_distributions}

In this section we explain some of the basic details for tests for equality of distributions, when they fail and set up the next section on different measures of distributional equality. 

% subsection two_sample_tests_for_equality_of_distributions (end)


\subsection{Test Statistics for Distributional equality} % (fold)
\label{sub:test_statistics_for_distributional_equality}

Here we explore different measures of distributional similarity, KS statistic, AD statistic, Earth Movers, KL, etc. We can discuss their benefits and drawbacks in the context of testing. 

For example, we could calculate the KS distance with the observed noisy data, i.e. 

\begin{align*}
    \Phi(\tilde{X}, \tilde{Y}) = KS(\hat{F}_{\tilde{X}}, \hat{F}_{\tilde{Y}})
\end{align*}

This would compare the empirical distributions of the noisy data and return small values for cases where $\hat{F}_{\tilde{X}}$ is close to $\hat{F}_{\tilde{Y}}$ in the KS sense. 

We can also discuss their relationship to particular form of deconvolution and discuss how to include a deconvolution step in implementing $\Phi$. For example 
\begin{align*}
\Phi(\tilde{X}, \tilde{Y}) = KS(\decon(\Xtilde; \sigma_X), \decon(\Ytilde; \sigma_Y))    
\end{align*}

Here, we would be accounting for calculating the distances between the "correct" distributions because $\decon(\Xtilde; \sigma_X)$ is approximating $F_X$ and this comparison will be similar to the ideal noiseless compatison $\Phi(X, Y) = KS(\hat{F}_X, \hat{F}_Y)$. 

% subsection test_statistics_for_distributional_equality (end)


\subsection{Putting the Pieces Together} % (fold)
\label{sub:putting_the_pieces_together}

As described in \autoref{sub:deconvolution}, deconvolution involves approximating an underlying distribution of a set of noisy observations when the error's distribution and magnitude are known. So, given the set of observations $\Xtilde$ and error magnitudes $\sigma_X$ we can approximate the underlying distribution $F_X$. We denote this application of the deconvolution procedure as
\begin{equation}
\Fhat_X = \decon(\Xtilde; \sigma_X).
\end{equation}

\begin{algorithm}[t]
\caption{Bootstrap test for $F_X=F_Y=F_0$, where $F_0$ is a pre-specified distribution that can be sampled from.
The test statistic $\Phi(\cdot, \cdot)$ is also pre-specified.
\label{algo:bootstraptest}
}

 \KwData{$\Xtilde, \Ytilde$, $\sigma_X, \sigma_Y$}
 \KwResult{p-value: $p$}

 $\phi_{obs} = \Phi(\Xtilde, \Ytilde)$\\

 \For{$b$ in $1, ..., B$}{
    \For{$i$ in $1, ..., n_X$}{
        $X_i^{(b)} \sim F_0 \quad\text{and}\quad
        \varepsilon_{X,i}^{(b)} \sim N\left(0, \sigma^2_{X,i}\right)$\\
        $\Xtilde_i^{(b)} = X_i^{(b)} +\varepsilon_{X,i}^{(b)}$
    }
    \For{$j$ in $1, ..., n_Y$}{
        $Y_j^{(b)} \sim F_0 \quad\text{and}\quad
        \varepsilon_{Y,j}^{(b)} \sim N\left(0, \sigma^2_{Y,j}\right)$\\
        $\Ytilde_j^{(b)} = Y_j^{(b)} +\varepsilon_{Y,j}^{(b)}$
    }
    $\phi^{(b)} = \Phi(\Xtilde^{(b)}, \Ytilde^{(b)})$

 }
 $p = \frac{1}{B}\sum_{i = 1}^B {{\mathds{1}}\{\phi_{obs} > \phi^{(b)}\}}$

\end{algorithm}

\begin{algorithm}[t]
\caption{Parametric Bootstrap for Testing Equality of Distributions with known and normal noise distributions. It uses a pre-specified deconvolution function $\decon(\cdot; \cdot)$ and statistical distance metric $\Phi(\cdot, \cdot)$
\label{algo:deconvtest}
}

 \KwData{$\Xtilde, \Ytilde$, $\sigma_X, \sigma_Y$}
 \KwResult{p-value: $p$}

 $\Fhat_0 = \decon((\Xtilde; \Ytilde), (\sigma_X, \sigma_Y))$

 Return the $p$-value from \autoref{algo:bootstraptest} with $F_0 = \Fhat_0$.

\end{algorithm}

% subsection putting_the_pieces_together (end)

\subsection{Validity}
\label{sec:validity}

We turn to the question of the validity of the tests defined by \autoref{algo:bootstraptest}
and \autoref{algo:deconvtest},
where validity means that the condition $\alpha \leq \alpha^*$ is satisfied.
To be explicit, the test function under consideration is $\varphi(\Xtilde,\Ytilde) = \Ind\cbr{p \leq \alpha^*}$,
where $p$ is the output of each algorithm.


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
    \includegraphics[height=2in]{{pdiff}.pdf}
    \caption{
        Bootstrap test.
    \label{fig:pdiff}
    }
    \end{subfigure}
    \caption{
        Validity of the bootstrap test.
        \label{fig:validity}
    }
\end{figure}

In \autoref{algo:bootstraptest}, the boostrap is used to obtain the distribution of the test statistic under $F_X = F_Y = F_0$,
where $F_0$ is pre-specified.
It follows that if it is indeed the case that $F_X = F_Y = F_0$, then the $p$-value is approximately uniformly distributed (exactly as $B \to \infty$), 
and therefore $\power(F_0, F_0)=\alpha^*$.
However, it does not follow that $\alpha=\alpha^*$, 
as there may very well exist a different $F' \in \alldistribs$
for which $\power(F', F') > \alpha^*$.
We can decompose the significance \autoref{eq:significance} into
\begin{equation}
    \alpha =  \power(F_0, F_0) + \sup_{F' \in \alldistribs}
            \big( \underbrace{
                        \power(F', F') - \power(F_0,F_0) 
                  }_{\equiv \pdiff(F_0,F')} \big)
    \,,
\end{equation}
and the difference in power $\pdiff(F_0,F')$ can be seen to be the 
probability under $F'$ of the test statistic exceeding its $(1-\alpha^*)$
quantile under $F_0$ minus $\alpha^*$ (illustrated in \autoref{fig:pdiff}).
The implication is that it is desirable for the distribution of the test
statistic under the null to be as insensitive as possible to choice of $F_0$.
Asymptotically, this property is perfectly achieved by the Kolmogorov-Smirnov test statistic
\mr{is this true? what's the exact statement?}.

So far, we have assumed that $F_0$ is pre-chosen completely arbitrarily.
If the true null distribution $F_0^\star$ was known, we would simply choose
$F_0=F_0^\star$ which would guarantee $\alpha=\alpha^*$.
With $F_0^\star$ unknown, it remains desirable to choose an $\Fhat_0$ that is close to $F_0^\star$,
so that $\pdiff(\Fhat_0,F_0^\star)$ is small.
This is the aim of the deconvolution step in \autoref{algo:deconvtest}.


% section methods (end)



\section{Simulation Study} % (fold)
\label{sec:simulation_study}


\subsection{Homoskedastic Error} % (fold)
\label{sub:homoskedastic_error}

I claim this above:
Under homoskedastic noise, i.e. $\sigma^2_{X,i} = \sigma^2_{Y,i} = \sigma^2$, the distributions of the noisy observations are the same under the null distribution and hence the K-S statistic is valid. 

Claim:  

Is this true? Let's compare the relative power of using KS directly to using our proposed method.

% subsection homoskedastic_error (end)

\subsection{Deconvolve and Test?} % (fold)
\label{sub:deconvolve_and_test}

A separate method has been proposed for this [Paul Green paper]. We could apply deconvolution to the two sets of data separately 

\begin{align*}
	\Fhat_X = \decon(\Xtilde; \sigma_X)\\
	\Fhat_Y = \decon(\Ytilde; \sigma_Y)
\end{align*}
 and compare these two distributions directly. Our intuition is that this will be conservative and underpowered. It is unclear as to how this will perform under large samples -- let's investigate. Even if this performs well under large sample, hell even if this has the correct size when $n_x, n_Y \rightarrow \infty$, it is still a large sample argument. 

% subsection deconvolve_and_test (end)

\subsection{Heteroskedastic but equal} % (fold)
\label{sub:heteroskedastic_but_equal}

Heteroskedastic errors with equal distributions. Set it so that $\sigma_{X_i} \sim \sigma_{Y_j}$ with increasing variability of variances, i.e. start at homoskedatic and create increasingly variable variances. 


% subsection heteroskedastic_but_equal (end)


% section simulation_study (end)




\section{Future Work} % (fold)
\label{sec:future_work}

The choice of deconvolution and distribution comparison methods discussed in \autoref{sub:deconvolution} and \autoref{sub:test_statistics_for_distributional_equality} will absolutely have an impact on the power of the resulting test. For example, some deconvolution methods work under additional assumptions and hence, if those assumptions are correct, would have faster convergence rates. Under $H_0$ this would imply... F0 close to F0hat. This could be negated by ... tests that are insentitive to defficiencies in deconvolution methods. For example, the kernel-based methods have poor tail approximation behavior, but comparing some distance metrics, like earth movers(?) may be less sensitive to the tails and hence the resulting null distribution will not be affected. We shouold investigate this interaction further. 

% section future_work (end)



\bibliographystyle{unsrtnat}
\bibliography{references}



\end{document}
